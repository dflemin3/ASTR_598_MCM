{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: my functions here aren't complete, only drafts.  For fully-functional functions, check out the hw3.py scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import factorial\n",
    "import time\n",
    "\n",
    "#Typical plot parameters that make for pretty plots\n",
    "mpl.rcParams['figure.figsize'] = (8,8)\n",
    "mpl.rcParams['font.size'] = 15.0\n",
    "mpl.rc('text', usetex='true') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(int((time.time()%1)*1.0e8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GUI Forwarding when SSHing into Hyak\n",
    "\n",
    "`ssh -X (hyak)` tells hyak you want X11 stuff sent to your computer.\n",
    "\n",
    "`qsub -V -I` tells the scheduler that you want graphic stuff sent to you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rejection Method\n",
    "\n",
    "https://en.wikipedia.org/wiki/Rejection_sampling\n",
    "\n",
    "Rejection sampling is based on the observation that to sample a random variable one can sample uniformly from the region under the graph of its density function.\n",
    "\n",
    "It's effectively doing a Monte Carlo integration to find the area under the curve.\n",
    "\n",
    "Method to generate random variates that work for any distribution in any dimension!\n",
    "\n",
    "You have some distribution P(x) and you want to generate x0 according to the distribution.\n",
    "\n",
    "```\n",
    "1) Draw box around distribution from [0,x_max] and [0,P_max] so box bounds distribution.\n",
    "\n",
    "2) Sample uniform random number for both x, y from box bounds \n",
    "\n",
    "3) If a number doesn't fall under the distribution, reject it\n",
    "    if you sample (x0,y0) and if y0 < P(x0), accept, otherwise try again\n",
    "    \n",
    "4) Try it N number of times and you'll get something below the curve\n",
    "\n",
    "```\n",
    "\n",
    "Notes:\n",
    "- Method becomes inefficient if distribution is centrally peaked but a slow decline.\n",
    "- Works better for distributions that have quickly declining tails.\n",
    "\n",
    "Think of it this way:  Imagine you have a vary centrally peaked distribution with tails that die really quickly like a super narrow gaussian.  Any random (x0,y0) pair will be very unlikely to satisfy y0 < p(x0) in the tails since p(x0) is small, so not many x0 will be saved.  This recovered the distribution's behavior that not much stuff is out there.  If you hit the central peak, however, y0 < p(x0) will be likely to return true since p(x0) is large.  In this case, many x0 are returned recovering the behavior that the probability is high in that location.  The pileup of returned x0 is like the peak in the distribution (think histograms!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_YMAX_poisson(lam):\n",
    "    # Find the int that gives the max values\n",
    "    # of the poisson distribution P(n), YMAX\n",
    "    \n",
    "    val = 0.0\n",
    "    m = 0\n",
    "\n",
    "    while True:\n",
    "        tmp = poisson(m,lam)\n",
    "        \n",
    "        # If increasing, save value\n",
    "        if tmp > val:\n",
    "            val = tmp\n",
    "            m_max = m\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "        m = m + 1\n",
    "    # end while\n",
    "    \n",
    "    return m_max, val\n",
    "# end function\n",
    "    \n",
    "def find_XMAX_poisson(lam,m_max,eps=1.0e-8,MAX=50):\n",
    "    # Given the index for the maximum of the poisson function,\n",
    "    # find the index at which the Poisson distribution decays\n",
    "    # to near 0, XMAX\n",
    "\n",
    "    m = m_max\n",
    "    count = 0\n",
    "\n",
    "    # Start from peak and move rightward until distribution goes to 0\n",
    "    while count < MAX:\n",
    "        tmp = poisson(m,lam)\n",
    "        \n",
    "        # poisson is 0 enough\n",
    "        if tmp < eps:\n",
    "            return m\n",
    "        \n",
    "        m = m + 1\n",
    "        count = count + 1\n",
    "    \n",
    "    return -1.0        \n",
    "# end function\n",
    "   \n",
    "#####################\n",
    "#\n",
    "# Problem 1a\n",
    "#   \n",
    "#####################   \n",
    "   \n",
    "# Return n value drawn from the poisson\n",
    "# 1 dimensional case for the poisson distribution\n",
    "def nextPoisson(n,lam,MAX_ATTEMPTS=40):\n",
    "      \n",
    "    # Default inits    \n",
    "    m_max, YMAX = find_YMAX_poisson(lam) \n",
    "    XMAX = find_XMAX_poisson(lam,m_max)    \n",
    "    \n",
    "    ret = []    \n",
    "    \n",
    "    for i in range(0,n):\n",
    "        count = 0\n",
    "        while(count < MAX_ATTEMPTS):\n",
    "            x0 = np.random.uniform(low=0.0,high=XMAX)\n",
    "            y0 = np.random.uniform(low=0.0,high=YMAX)\n",
    "            \n",
    "            # Increment count\n",
    "            count += 1\n",
    "            \n",
    "            # Does it fall under the distribution\n",
    "            if y0 < poisson(x0,lam):\n",
    "                ret.append(x0)\n",
    "                break\n",
    "                \n",
    "            # too many attempts!\n",
    "            if count >= MAX_ATTEMPTS:\n",
    "                print(\"Error: Too many attempts while calling rejection_sampling!\")\n",
    "                ret.append(0.0)\n",
    "                break\n",
    "        #end while\n",
    "    \n",
    "    return np.asarray(ret)\n",
    "# end function\n",
    " \n",
    "#####################\n",
    "#\n",
    "# Problem 2a\n",
    "#   \n",
    "#####################\n",
    "           \n",
    "# Returns n random deviates drawn from a Gaussian\n",
    "# using the Box-Mueller Transform\n",
    "# where args = (mu,sigma)\n",
    "def nextGaussian(n,*args):\n",
    "\n",
    "    # Extra args\n",
    "    mu = args[0]\n",
    "    sigma = args[1]\n",
    "    \n",
    "    # x1, x2 are uniform randoms from [0,1]\n",
    "    # Draw 2 since BM transform returns 2 distributed according to normal distribution\n",
    "    x1 = np.random.uniform(size=int(n/2))\n",
    "    x2 = np.random.uniform(size=int(n/2))\n",
    "    U1 = 1.0 - x1\n",
    "    U2 = 1.0 - x2\n",
    "    \n",
    "    z0 = np.sqrt(-2.0 * np.log(U1)) * np.cos(2.0*np.pi * U2)\n",
    "    z1 = np.sqrt(-2.0 * np.log(U1)) * np.sin(2.0*np.pi * U2)\n",
    "    \n",
    "    # Returns vector of normally distributed data!\n",
    "    return np.concatenate([z0 * sigma + mu,z1 * sigma + mu])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling collisions/reflections in 3D spherical geometry\n",
    "Quick aside:\n",
    "\n",
    "P($\\theta,\\phi$)d$\\Omega$\n",
    "\n",
    "$\\mu = 2x_1 - 1$\n",
    "\n",
    "$\\phi = 2 \\pi x_2$\n",
    "\n",
    "where x1, x2 are sampled uniformly from [0,1] and $\\mu = \\cos(\\theta)$ and ranges from [-1,1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chain Monte Carlo\n",
    "\n",
    "A Markov chain is a random process that makes a transition from one state to another based on a value of a random variable.  The choice of the next state depends only on the current state and not on any previous state (memoryless).\n",
    "\n",
    "Example: Random walk\n",
    "```\n",
    "You have some grid where you have some probability to travel in one of 4 directions (1 = up, 2 = down, 3 = left, 4 = right).  You randomly choose which direction to go and the next state (location) only depends on the current one and the random number that decides the transition.\n",
    "```\n",
    "\n",
    "Not an Example: Self-avoiding random walk\n",
    "```\n",
    "Same as above, but you're not allowed to go back to where you have been.  Not a Markov chain BECAUSE it depends on previous states, that is, where you have already been.  Simulations of polymers employ this technique.\n",
    "```\n",
    "\n",
    "Example:\n",
    "```\n",
    "(state 1) --0.3--> (state 2)          (state 3)\n",
    "        --->\n",
    "          -0.7->         \n",
    "             --->  (state x)\n",
    "             \n",
    "To see if you go from 1->2 or 1->x, sample uniformly from [0,1] to decide what transition to make.\n",
    "```\n",
    "\n",
    "In general, you have some starting state and potentially many different ending states.  Each transition from current state to some ending state has some probability where the total sums to 1, of course.  The choice of next state depends on some random variable.\n",
    "\n",
    "To accomplish this, we can define a transition matrix. For a 3 state system, it would look like the following:\n",
    "```\n",
    "T = | P11 P12 P13 |\n",
    "    | P21 P22 P23 |\n",
    "    | P31 P32 P33 |\n",
    "\n",
    "where P12 is the probability of transitions from 1->2.  In general P12 != P21 (!!).\n",
    "```\n",
    "\n",
    "Example: Random walk transition matrix\n",
    "To accomplish this, we can define a transition matrix. For a 3 state system, it would look like the following:\n",
    "```\n",
    "T = | 1/4 0 |\n",
    "    | 0  3/4|\n",
    "In this example using the convention above for [[1,2],[3,4]] 2x2 transition matrix, you could only go up or right, but mostly likely right.\n",
    "```\n",
    "In this example, each time you run the simulation, you get a some number of steps.  The average number of steps goes as sqrt(N) for N transitions.  And this doesn't depend on the dimensions!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use of random walk to solve Laplace PDE at a single point.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Laplace%27s_equation\n",
    "\n",
    "$$\n",
    "\\frac{\\delta^2 u}{\\delta x^2} + \\frac{\\delta^2 u}{\\delta y^2} = 0\n",
    "$$\n",
    "with u given on some boundary.\n",
    "\n",
    "You want to evaluate this at (x0,y0) and there's some boundary ub.  People have shown that\n",
    "$$\n",
    "u(x0,y0) = \\frac{1}{N} \\sum_i^N u_{b,i}\n",
    "$$\n",
    "where this is done where N is the number of walkers who random walk (markov chain style) to some boundary point.  So it's not like an average of ALL boundary points, but just the ones that the walkers explore.  So what happens is some random walker starts at (x0,y0) and uniformly random walks (think diffusion) through some grid until it hits some boundary point u_b,i.  Save that value for the sum later.\n",
    "\n",
    "This technique is embarrassingly parallel (1 walker per core! Map reduce that!). You map the walkers to a bunch of machines, then reduce by doing the sums.  This technique is also great because it can handle an arbitrary boundary (as long as that boundary is well-known, of course)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multicore hyak (supercomputer) usage\n",
    "\n",
    "How to run multicore jobs on 1 node using hyak.  This will be useful later on when we want to have a lot of walkers or something and we want to compute the mean and get a good (small) variance.\n",
    "\n",
    "For batch jobs, write a PBS script (many examples on the hyak wiki).  Once you have a script ready to go, on a login node you call\n",
    "```\n",
    "qsub my_script.pbs\n",
    "```\n",
    "to run your stuff on an entire node, depending on what you ask for.\n",
    "\n",
    "In the PBS script, include\n",
    "```\n",
    "cat mytasks | parallel\n",
    "```\n",
    "\n",
    "where mytaks is a file that has\n",
    "```\n",
    "mytask 1\n",
    "mytask 2\n",
    "...\n",
    "mytask n\n",
    "```\n",
    "\n",
    "where each `mytask i` is a command to run, like python run_random_walk.py.\n",
    "\n",
    "Parallel is smart and will run as many tasks at a time as there are cores.  To save output, make sure each task writes results to a certain place. So this stuff covers the map part, but you'll need to do the reduce step later.  If using the backfill queue, make sure to include check in your run.py or whatever to make sure that if it has already ran to not run it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
